# -*- coding: utf-8 -*-
"""lematizador.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/delgadojoseph271/chat_bot/blob/develop/lematizador.ipynb
"""

import nltk

'''nltk.download('punkt')
nltk.download('wordnet')
nltk.download('universal_tagset')
nltk.download('spanish_grammars')
nltk.download('tagsets')
nltk.download('stopwords') # faltaaaaa
nltk.download('omw-1.4')'''

import json

def guardar_json(datos):
    archivo=open("intents.json","w")
    json.dump(datos,archivo,indent=4)

biblioteca={"intents":
            [
                {"tag":"saludos",
                 "patterns":["hola",
                             "buenos dias",
                             "buenas tardes",
                             "buenas noches",
                             "como estas",
                             "hay alguien ahi?",
                             "hey",
                             "saludos",
                             "que tal"                      
                             ],
                 "responses":["hola soy rix, ya tienes en mente que auto quieres?"
                             ],
                 "context":[""]
                 },
                
                {"tag":"despedidas",
                 "patterns":["chao",
                             "adios",
                             "hasta luego",
                             "nos vemos",
                             "bye",
                             "hasta pronto",
                             "hasta la proxima"
                             ],
                 "responses":["hasta luego, tenga un buen dia",
                 "ha sido un placer, vuelva pronto"
                             ],
                 "context":[""]
                 },
                {"tag":"agradecimientos",
                 "patterns":["gracias",
                             "muchas gracias",
                             "mil gracias",
                             "muy amable",
                             "se lo agradezco",
                             "fue de ayuda",
                             "gracias por la ayuda",
                             "muy agradecido",
                             "gracias por su tiempo",
                             "ty"
                            ],
                 "responses":["de nada",
                              "feliz por ayudarlo",
                              "gracias a usted",
                              "estamos para servirle",
                              "fue un placer"
                             ],
                 "context":[""]
                },
                {"tag":"norespuesta",
                 "patterns":[""],
                 "responses":["no se detecto una respuesta"
                             ],
                 "context":[""]                    
                },
             {"tag":"duda",
                 "patterns":["nose",
                             "no estoy seguro",
                             "no lo eh pensado",
                             "ni idea"
                             "que me recomiendas"],
                 "responses":["no hay problema, si no estas seguro empezemos con unas preguntas rutinarias para recomendarte el mejor auto"
                             ],
                 "context":[""]
              },
             {"tag":"pregunta_sobre_bodega",
                 "patterns":["que autos tienen",
                             "que carros tienen",
                             "que autos venden",
                             "cuales tienen en unidad"
                             "nose cuales autos tienes",
                             "muestramelos que tienen "],
                 "responses":["mostrar bodega,en texto o imagenes"
                             ],
                 "context":[""]
              },
                 
            ] 
        }

guardar_json(biblioteca)

import json
import pickle
import numpy as np

import nltk

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, Flatten, Dense,Dropout
from tensorflow.keras.optimizers import SGD


ignore_words=["?","¿","!","¡"]
data_file= open("intents.json").read() #aqui cargo el archivo en formato json
intents = json.loads(data_file) # aqui convierto el archivo json a diccionario
intents

words=[]
classes=[]
documents=[]

for intent in intents["intents"]: #accedo a la lista de diccionarios
    for pattern in intent["patterns"]: # accedo a la lista de palabraas
        
        
        #tokenizar cada palabra
        
        w=nltk.word_tokenize(pattern) #separamos las oraciones palabra por palabra y guardamos cada palabra como token
        words.extend(w)
        
        #agrego un array de documentos
        documents.append((w,intent["tag"]))
        
        #añadimos clases  a nuestra lista de clases
        if intent["tag"] not in classes:
            classes.append(intent["tag"])

print(words)

print("\n ####################################################################################################### \n")

print(documents)

print("\n ####################################################################################################### \n")

print(classes)

from nltk.stem import SnowballStemmer
stemmer = SnowballStemmer('spanish')
words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]

print("esta lista words seran las palabras LEMATIZADAS de REFERENCIA")
print(words)


pickle.dump(words,open("words.pkl","wb"))
pickle.dump(classes,open("classes.pkl","wb"))

training=[]
output_empty=[0]*len(classes)# creamos una matriz del numero de patterns con valor inicial 0
                            # creamos una matriz que tenga tantas columnas como classes

for doc in documents: #en doc esta la raw_data -> datos sin procesar
#doc[0] -> tokens, o palabras
#doc[1] -> tag -> clase
    
    
    #bag of words
    bag=[]
    #lista de tokens
    pattern_words=doc[0]# doc[0] es la lista de palabras
    # lematizacion del token
#CAMBIO!!!!    pattern_words= [lemmatizer.lemmatize(word.lower()) for word in pattern_words  if word not in ignore_words ]
    pattern_words= [stemmer.stem(word.lower()) for word in pattern_words  if word not in ignore_words ]
    
    
    # si la palabra coincide introduzco 1, en caso contrario 0
    
    for palabra in words:
        bag.append(1) if palabra in pattern_words else bag.append(0) 
        #si la palabra de referencia esta dentro de pattern_words ponga 1
        #print(bag)
    
    output_row =list(output_empty)
    output_row[classes.index(doc[1])] = 1 #doc en la posicion 1 es el pattern
                #busca en que posicion esta el tag y pone un 1 en esa posicion del output_row
                #ejemplo si es saludo pone [1,0,0,0]
    
    training.append([bag,output_row])
    #print(output_row)

print(training)
print(len(training))
training=np.array(training,dtype=object) # cambiamos la lista de listas a un formato numpy.array

print(f"tendremos como entradas {len(training[0][0])} columnas")
#print(f"tendremos como entradas {} columnas".format(len(training[0][0])))      

print(f"tendremos como salidas {len(training[0][1])} columnas")      
print()      
print(training)

x_train= list(training[:,0]) #asi porque estamos en formato numpy.array ||| training[inicio:fin,index]
y_train= list(training[:,1])

model = Sequential()

model.add(Dense(128, input_shape=(len(x_train[0]),), activation='relu')) #añadimos 1 capa: entrada de datos
model.add(Dropout(0.5))
model.add(Dense(64,activation='relu')) #capa oculta -> aprendizaje
model.add(Dropout(0.5))
model.add(Dense(len(y_train[0]),activation='softmax')) # capa de salida toma de desiciones

#El (DROPOUT) es una técnica en la que se ignoran neuronas seleccionadas al azar 
#durante el entrenamiento. Se "descartan" aleatoriamente. 
#Esto significa que su contribución a la activación de las neuronas descendentes 
#se elimina temporalmente en el paso hacia delante, y cualquier actualización de
#peso no se aplica a la neurona en el paso hacia atrás.


# SGD es un optimizador estocástico de descenso gradiente. 
# Incluye soporte para momentum, decaimiento de la tasa de aprendizaje y momentum Nesterov.

# SGD optimiza los parametros


sgd=SGD(learning_rate=0.01,decay=1e-6,momentum=0.9,nesterov=True) 

model.compile(loss="categorical_crossentropy",optimizer=sgd,metrics=["accuracy"])

#le mando los datos de train para que entrene y aprenda
#fit ajusta los datos para crear un modelo (Sequential de 3 capas) que pueda predecir los datos

hist=model.fit(np.array(x_train),np.array(y_train),epochs=300,batch_size=5,verbose=1)
model.save("chatbot_model.h5",hist)
print("modelo creado")

import nltk, json,pickle
import numpy as np

from nltk.stem import SnowballStemmer
from tensorflow.keras.models import load_model
stemmer = SnowballStemmer('spanish')


model=load_model("chatbot_model.h5")
intents= json.loads(open("intents.json").read())
words=pickle.load(open("words.pkl","rb"))
classes=pickle.load(open("classes.pkl","rb"))

import nltk, json,pickle
import numpy as np

from nltk.stem import SnowballStemmer
from tensorflow.keras.models import load_model
stemmer = SnowballStemmer('spanish')


model=load_model("chatbot_model.h5")
intents= json.loads(open("intents.json").read())
words=pickle.load(open("words.pkl","rb"))
classes=pickle.load(open("classes.pkl","rb"))

def clean_up_sentence(sentence):
    # tokenizar la oracion
    sentence_words=nltk.word_tokenize(sentence) # tokenizamos
    sentence_words=[stemmer.stem(word.lower()) for word in sentence_words] #lematizamos
    return sentence_words


def bow (sentence,words,show_details=True): #lazo entre lo que ingreso el usuario tokenizado y la referencia 
    sentence_words=clean_up_sentence(sentence)
    
    bag=[0]*len(words)
    
    for i in sentence_words:
        for j,w in enumerate(words):
            if w==i: # asigna 1 si la palabra actual está en la posición del vocabulario 
                bag[j]=1
                if show_details:
                    print("encontrado en la bolsa: ",w)
    return (np.array(bag))

def predict_class(sentence,model):
    # filtrar las predicciones  por debajo del umbral
    p = bow(sentence,words,show_details=False) # retorno del bag # p porque es el preprocesamiento
    res = model.predict(np.array([p]))[0] # res es la eficacia, o probabilidad de que la palabra sea de algun tipo
    #model.predict me retorna el % eficacia  , ejm 60% saludo
    # [0] es la palabra , [1] es el tag
    
    
    ERROR_THRESHOLD=0.25 #UMBRAL
    
    
    # a results le llega  [1,0,0,0]
    results= [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD] 
    # si la probabilidad es > 25% determinela como resultado correcto
    
    #r[0]= tag
    #r[1]= probabilidad
    
    #ordenar por peso de la probabilidad
    results.sort(key=lambda x: x[1], reverse=True)  
    #ordena de mayor a menor la probabilidad de que el resultado sea acertado
    return_list = []    
    for r in results:   
        return_list.append({"intent": classes[r[0]], "probability": str(r[1])})   
    print("print de return list: ", return_list)  ##  me dice de que tipo es y cual es la probabilidad de que sea correcto
    return return_list

import random

def get_response(ints,intents_json): # obtiene una respuesta aleatoria segun el ints correspondiente a lo que ingreso el usuario
    tag= ints[0]["intent"] # obtenemos cual era el tag  segun lo que ingreso el usuario ints[0]
    list_of_intents=intents_json["intents"] # sacamos la lista de intents de referencia
    
    for i  in list_of_intents: 
        if (i["tag"]==tag): #miramos donde coincide el tag del sentence con la referencia
            result= random.choice(i["responses"]) # random.choice, toma un elemento aleatorio de la lista
            break
    return result
    
# este es el metodo principal, aqui nace todo
def chatbot_response(text): 
    ints=predict_class(text,model) #ints es el intents que creamos a apartir de lo que ingreso el usuario
    res=get_response(ints,intents)# intents es el json de referencia
    return res
    
    
######################## SOLO PARA PROBARLO EN CONSOLA ##########################
# "main"
texto_us="" # lo que ingresa el usuario
print(" bienvenido, para salir  escriba salir \n")

while texto_us!="salir":
    texto_us=input()
    res=chatbot_response(texto_us)
    print(res)

